{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgsYxHYGWkdk",
        "outputId": "6c02658d-e7d5-4f60-ea12-8ed66e06cb81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Password: ··········\n"
          ]
        }
      ],
      "source": [
        "# connect github\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "user = 'peterrayshen'\n",
        "password = getpass('Password: ')\n",
        "repo_name = 'syde552-project'\n",
        "# your password is converted into url format\n",
        "password = urllib.parse.quote(password)\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git'.format(user, password, repo_name)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "# Bad password fails silently so make sure the repo was copied\n",
        "assert os.path.exists(f\"/content/{repo_name}\"), \"Incorrect Password or Repo Not Found, please try again\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"syde552-project/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCh_BAtucaCz",
        "outputId": "15821a48-a85d-4eff-8aef-3113019b07e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rk4PtlIWkdo",
        "outputId": "5d87bfc9-e59d-45fb-ce40-8e80aca7abf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (1.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install fastprogress\n",
        "from fastprogress import master_bar, progress_bar\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "import pickle\n",
        "import argparse\n",
        "\n",
        "from utils import SurrGradSpike, get_shd_dataset, sparse_data_generator_from_hdf5_spikes, sparse_data_generator_from_hdf5_spikes_2\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from constants import xp_type_tau_constant, xp_type_tau_gauss, xp_type_tau_uniform, nb_inputs, nb_hidden, nb_outputs, time_step, nb_steps, max_time, batch_size, tau_mem_readout, tau_syn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sa-nOvg9Wkdp"
      },
      "outputs": [],
      "source": [
        "xp_type = xp_type_tau_constant\n",
        "\n",
        "# general xp params\n",
        "nb_epochs = 10\n",
        "lr = 1e-3\n",
        "num_trials = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "M_QuxKTUqkdD",
        "outputId": "31e12031-404c-4e54-aab0-717c7da2045b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if xp_type == xp_type_tau_uniform:\n",
        "    xp_params = {\n",
        "        'uniform_lower_bounds': np.array([38, 35, 30, 20]) * 1e-3,\n",
        "        'uniform_upper_bounds':  np.array([42, 45, 50, 60]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "elif xp_type == xp_type_tau_gauss:\n",
        "    xp_params = {\n",
        "        'gauss_mean': np.array([40, 40, 40, 40]) * 1e-3,\n",
        "        'gauss_std':  np.array([2, 5, 10, 20]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "elif xp_type == xp_type_tau_constant:\n",
        "    xp_params = {\n",
        "        'constant_tau': np.array([20, 40, 60, 80]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "else:\n",
        "    raise ValueError('xp type not recognized')\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Check whether a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")     \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "train_file = h5py.File('/content/gdrive/MyDrive/Colab Notebooks/syde552/data/shd_train.h5', 'r')\n",
        "test_file = h5py.File('/content/gdrive/MyDrive/Colab Notebooks/syde552/data/shd_test.h5', 'r')\n",
        "\n",
        "x_train = train_file['spikes']\n",
        "y_train = train_file['labels']\n",
        "x_test = test_file['spikes']\n",
        "y_test = test_file['labels']\n",
        "\n",
        "x_train_df = pd.DataFrame()\n",
        "x_train_df['times'] = np.array(x_train['times'])\n",
        "x_train_df['units'] = np.array(x_train['units'])\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_train_train, x_train_valid, y_train_train, y_train_valid = train_test_split(x_train_df, y_train_np, test_size=0.2, random_state=42, stratify=y_train_np)\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta_readout    = float(np.exp(-time_step/tau_mem_readout))\n",
        "\n",
        "xp_results = []\n",
        "\n",
        "batch_cache_train_train = list(sparse_data_generator_from_hdf5_spikes_2(x_train_train, y_train_train, batch_size, nb_steps, nb_inputs, max_time, device))\n",
        "batch_cache_train_valid = list(sparse_data_generator_from_hdf5_spikes_2(x_train_valid, y_train_valid, batch_size, nb_steps, nb_inputs, max_time, device))\n",
        "batch_cache_test = list(sparse_data_generator_from_hdf5_spikes(x_test, y_test, batch_size, nb_steps, nb_inputs, max_time, device, shuffle=False))\n",
        "\n",
        "\n",
        "def get_mini_batch(x_data, y_data, shuffle=False):\n",
        "    for ret in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, device, shuffle=shuffle):\n",
        "        return ret \n",
        "x_batch_mini, _ = get_mini_batch(x_test, y_test)\n",
        "\n",
        "for trial_num in range(num_trials):\n",
        "    for i in range(xp_params['num_xps']):\n",
        "        if xp_type == xp_type_tau_uniform:\n",
        "            uniform_lower = xp_params['uniform_lower_bounds'][i]\n",
        "            uniform_upper = xp_params['uniform_upper_bounds'][i]\n",
        "            gen_tau_np = np.tile(np.random.uniform(low=uniform_lower, high=uniform_upper, size=nb_hidden), (batch_size, 1))\n",
        "            print(f\"init done for lower bound {uniform_lower}, upper bound {uniform_upper}\")\n",
        "        elif xp_type == xp_type_tau_gauss:\n",
        "            gauss_mean = xp_params['gauss_mean'][i]\n",
        "            gauss_std = xp_params['gauss_std'][i]\n",
        "            gen_tau_np = np.tile(np.random.normal(loc=gauss_mean, scale=gauss_std, size=(nb_hidden)), (batch_size, 1))\n",
        "            print(f\"init done for mean {gauss_mean}, stddev {gauss_std}\")\n",
        "        elif xp_type == xp_type_tau_constant:\n",
        "            constant_tau = xp_params['constant_tau'][i]\n",
        "            gen_tau_np = np.ones((batch_size,nb_hidden)) * constant_tau\n",
        "            print(f\"init done for constant tau {constant_tau}\")\n",
        "        else:\n",
        "            raise ValueError('xp type not recognized')\n",
        "\n",
        "        gen_tau_np[gen_tau_np < 5e-3] = 5e-3\n",
        "        beta_np = np.exp(-time_step/gen_tau_np)\n",
        "        beta_torch = torch.from_numpy(beta_np).float().to(device=device)\n",
        "\n",
        "        weight_scale = 0.2\n",
        "\n",
        "        w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "        w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "        v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(v1, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "        spike_fn  = SurrGradSpike.apply\n",
        "\n",
        "        def run_snn(inputs):\n",
        "            syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "            mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "            mem_rec = []\n",
        "            spk_rec = []\n",
        "\n",
        "            # Compute hidden layer activity\n",
        "            out = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "            h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "            for t in range(nb_steps):\n",
        "                h1 = h1_from_input[:,t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
        "                mthr = mem-1.0\n",
        "                out = spike_fn(mthr)\n",
        "                rst = out.detach() # We do not want to backprop through the reset\n",
        "\n",
        "                new_syn = alpha*syn +h1\n",
        "                new_mem =(beta_torch*mem +syn)*(1.0-rst)\n",
        "\n",
        "                mem_rec.append(mem)\n",
        "                spk_rec.append(out)\n",
        "                \n",
        "                mem = new_mem\n",
        "                syn = new_syn\n",
        "\n",
        "            mem_rec = torch.stack(mem_rec,dim=1)\n",
        "            spk_rec = torch.stack(spk_rec,dim=1)\n",
        "\n",
        "            # Readout layer\n",
        "            h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
        "            flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "            out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "            out_rec = [out]\n",
        "            for t in range(nb_steps):\n",
        "                new_flt = alpha*flt +h2[:,t]\n",
        "                new_out = beta_readout*out +flt\n",
        "\n",
        "                flt = new_flt\n",
        "                out = new_out\n",
        "\n",
        "                out_rec.append(out)\n",
        "\n",
        "            out_rec = torch.stack(out_rec,dim=1)\n",
        "            other_recs = [mem_rec, spk_rec]\n",
        "            return out_rec, other_recs\n",
        "\n",
        "        def compute_classification_accuracy(batch_cache):\n",
        "            \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "            accs = []\n",
        "            for x_local, y_local in batch_cache_test:\n",
        "                output,_ = run_snn(x_local.to_dense())\n",
        "                m,_= torch.max(output,1) # max over time\n",
        "                _,am=torch.max(m,1)      # argmax over output units\n",
        "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "                accs.append(tmp)\n",
        "            return np.mean(accs)\n",
        "\n",
        "        def compute_classification_accuracy_2(batch_cache, device):\n",
        "            \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "            accs = []\n",
        "            for x_local, y_local in batch_cache:\n",
        "                output,_ = run_snn(x_local.to_dense())\n",
        "                m,_= torch.max(output,1) # max over time\n",
        "                _,am=torch.max(m,1)      # argmax over output units\n",
        "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "                accs.append(tmp)\n",
        "            return np.mean(accs)\n",
        "\n",
        "        def train_with_validation(\n",
        "            x_train_train, \n",
        "            y_train_train,\n",
        "            x_train_valid,\n",
        "            y_train_valid, \n",
        "            lr=1e-3, \n",
        "            nb_epochs=10,\n",
        "            ):\n",
        "            params = [w1,w2,v1]\n",
        "            optimizer = torch.optim.Adamax(params, lr=lr, betas=(0.9,0.999))\n",
        "\n",
        "            log_softmax_fn = nn.LogSoftmax(dim=1)\n",
        "            loss_fn = nn.NLLLoss()\n",
        "            \n",
        "            loss_train_supervised = []\n",
        "            loss_train_reg = []\n",
        "            loss_valid_supervised = []\n",
        "            loss_valid_reg = []\n",
        "            acc_train = []\n",
        "            acc_valid = []\n",
        "\n",
        "            for e in range(nb_epochs):\n",
        "                # train\n",
        "                local_loss_train_supervised = []\n",
        "                local_loss_train_reg = []\n",
        "                for x_local, y_local in batch_cache_train_train:\n",
        "                    output,recs = run_snn(x_local.to_dense())\n",
        "                    _,spks=recs\n",
        "                    m,_=torch.max(output,1)\n",
        "                    log_p_y = log_softmax_fn(m)\n",
        "                    \n",
        "                    # Here we set up our regularizer loss\n",
        "                    # The strength paramters here are merely a guess and there should be ample room for improvement by\n",
        "                    # tuning these paramters.\n",
        "                    reg_loss = 2e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
        "                    reg_loss += 2e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
        "                    \n",
        "                    # Here we combine supervised loss and the regularizer\n",
        "                    loss_val = loss_fn(log_p_y, y_local) + reg_loss\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss_val.backward()\n",
        "                    optimizer.step()\n",
        "                    local_loss_train_supervised.append(loss_fn(log_p_y, y_local).item())\n",
        "                    local_loss_train_reg.append(reg_loss.item())\n",
        "                mean_supervised_loss = np.mean(local_loss_train_supervised)\n",
        "                mean_valid_loss = np.mean(local_loss_train_reg)\n",
        "                loss_train_supervised.append(mean_supervised_loss)\n",
        "                loss_train_reg.append(mean_valid_loss)\n",
        "                local_acc_train = compute_classification_accuracy_2(batch_cache_train_train, device)\n",
        "                acc_train.append(local_acc_train)\n",
        "                print(\"Epoch %i: train loss=%.5f, train acc=%.5f\"%(e+1,mean_supervised_loss + mean_valid_loss, local_acc_train))\n",
        "\n",
        "                # validation\n",
        "                local_loss_valid_supervised = []\n",
        "                local_loss_valid_reg = []\n",
        "                for x_local, y_local in batch_cache_train_valid:\n",
        "                    output,recs = run_snn(x_local.to_dense())\n",
        "                    _,spks=recs\n",
        "                    m,_=torch.max(output,1)\n",
        "                    log_p_y = log_softmax_fn(m)\n",
        "                    \n",
        "                    # Here we set up our regularizer loss\n",
        "                    # The strength paramters here are merely a guess and there should be ample room for improvement by\n",
        "                    # tuning these paramters.\n",
        "                    reg_loss = 2e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
        "                    reg_loss += 2e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
        "                    \n",
        "                    local_loss_valid_supervised.append(loss_fn(log_p_y, y_local).item())\n",
        "                    local_loss_valid_reg.append(reg_loss.item())\n",
        "                mean_supervised_loss = np.mean(local_loss_valid_supervised)\n",
        "                mean_valid_loss = np.mean(local_loss_valid_reg)\n",
        "                loss_valid_supervised.append(mean_supervised_loss)\n",
        "                loss_valid_reg.append(mean_valid_loss)\n",
        "                local_acc_valid = compute_classification_accuracy_2(batch_cache_train_train, device)\n",
        "                acc_valid.append(local_acc_valid)\n",
        "                print(\"Epoch %i: validation loss=%.5f, validation acc=%.5f\"%(e+1,mean_supervised_loss + mean_valid_loss, local_acc_valid))\n",
        "            \n",
        "            return loss_train_supervised, loss_valid_supervised, loss_train_reg, loss_valid_reg, acc_train, acc_valid\n",
        "\n",
        "        loss_supervised_train, loss_supervised_valid, loss_reg_train, loss_reg_valid, acc_train, acc_valid = train_with_validation(x_train_train, y_train_train, x_train_valid, y_train_valid, lr=lr, nb_epochs=nb_epochs)\n",
        "\n",
        "        output, other_recordings = run_snn(x_batch_mini.to_dense())\n",
        "        mem_rec, spk_rec = other_recordings\n",
        "        acc_test = compute_classification_accuracy(batch_cache_test)\n",
        "\n",
        "        xp_res = {\n",
        "            'w1': w1.cpu().detach().numpy(),\n",
        "            'w2': w2.cpu().detach().numpy(),\n",
        "            'v1': v1.cpu().detach().numpy(),\n",
        "            'weight_scale': weight_scale,\n",
        "            'xp_type': xp_type,\n",
        "            'xp_params': xp_params,\n",
        "            'i': i,\n",
        "            'gen_distribution_tau': gen_tau_np,\n",
        "            'gen_distribution_beta': beta_np,\n",
        "            'alpha': alpha,\n",
        "            'beta_readout': beta_readout,\n",
        "            'nb_inputs': nb_inputs,\n",
        "            'nb_hidden': nb_hidden,\n",
        "            'nb_outputs': nb_outputs,\n",
        "            'time_step': time_step,\n",
        "            'nb_steps': nb_steps,\n",
        "            'max_time': max_time,\n",
        "            'batch_size': batch_size,\n",
        "            'tau_mem_readout': tau_mem_readout,\n",
        "            'tau_syn': tau_syn,\n",
        "            'lr': lr,\n",
        "            'loss_supervised_train': loss_supervised_train,\n",
        "            'loss_supervised_valid': loss_supervised_valid,\n",
        "            'loss_reg_train': loss_reg_train,\n",
        "            'loss_reg_valid': loss_reg_valid,\n",
        "            'acc_test': acc_test,\n",
        "            'acc_train': acc_train,\n",
        "            'acc_valid': acc_valid,\n",
        "            'output': output.cpu().detach().numpy(),\n",
        "            'mem_rec': mem_rec.cpu().detach().numpy(),\n",
        "            'spk_rec': spk_rec.cpu().detach().numpy(),\n",
        "            'trial_num': trial_num,\n",
        "        }\n",
        "        xp_results.append(xp_res)\n",
        "\n",
        "\n",
        "with open(f'/content/gdrive/MyDrive/Colab Notebooks/syde552/learn_curve_{xp_type}.pkl', 'wb') as f:\n",
        "    pickle.dump(xp_results, f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgM07N16eMuP",
        "outputId": "8aeb2552-e228-440f-8b00-43f1e62dcc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "init done for constant tau 0.02\n",
            "Epoch 1: train loss=33.28828, train acc=0.07750\n",
            "Epoch 1: validation loss=6.66930, validation acc=0.07750\n",
            "Epoch 2: train loss=3.46378, train acc=0.07500\n",
            "Epoch 2: validation loss=2.89085, validation acc=0.07500\n",
            "Epoch 3: train loss=2.86525, train acc=0.09062\n",
            "Epoch 3: validation loss=2.84467, validation acc=0.09062\n",
            "Epoch 4: train loss=2.79278, train acc=0.11937\n",
            "Epoch 4: validation loss=2.75168, validation acc=0.11937\n",
            "Epoch 5: train loss=2.73507, train acc=0.12484\n",
            "Epoch 5: validation loss=2.73164, validation acc=0.12484\n",
            "Epoch 6: train loss=2.70633, train acc=0.13984\n",
            "Epoch 6: validation loss=2.69569, validation acc=0.13984\n",
            "Epoch 7: train loss=2.66720, train acc=0.16594\n",
            "Epoch 7: validation loss=2.64808, validation acc=0.16594\n",
            "Epoch 8: train loss=2.62048, train acc=0.17953\n",
            "Epoch 8: validation loss=2.61556, validation acc=0.17953\n",
            "Epoch 9: train loss=2.57294, train acc=0.20125\n",
            "Epoch 9: validation loss=2.56636, validation acc=0.20125\n",
            "Epoch 10: train loss=2.52289, train acc=0.20609\n",
            "Epoch 10: validation loss=2.52408, validation acc=0.20609\n",
            "init done for constant tau 0.04\n",
            "Epoch 1: train loss=69.10440, train acc=0.10359\n",
            "Epoch 1: validation loss=31.40644, validation acc=0.10359\n",
            "Epoch 2: train loss=21.13234, train acc=0.15266\n",
            "Epoch 2: validation loss=13.30535, validation acc=0.15266\n",
            "Epoch 3: train loss=9.62690, train acc=0.09203\n",
            "Epoch 3: validation loss=7.01021, validation acc=0.09203\n",
            "Epoch 4: train loss=5.88579, train acc=0.16062\n",
            "Epoch 4: validation loss=4.97375, validation acc=0.16062\n",
            "Epoch 5: train loss=4.40400, train acc=0.19844\n",
            "Epoch 5: validation loss=3.62237, validation acc=0.19844\n",
            "Epoch 6: train loss=2.79754, train acc=0.24531\n",
            "Epoch 6: validation loss=2.46633, validation acc=0.24531\n",
            "Epoch 7: train loss=2.38768, train acc=0.29531\n",
            "Epoch 7: validation loss=2.36386, validation acc=0.29531\n",
            "Epoch 8: train loss=2.26103, train acc=0.32750\n",
            "Epoch 8: validation loss=2.23736, validation acc=0.32750\n",
            "Epoch 9: train loss=2.18608, train acc=0.34531\n",
            "Epoch 9: validation loss=2.17677, validation acc=0.34531\n",
            "Epoch 10: train loss=2.11422, train acc=0.35922\n",
            "Epoch 10: validation loss=2.12880, validation acc=0.35922\n",
            "init done for constant tau 0.06\n",
            "Epoch 1: train loss=57.68862, train acc=0.06719\n",
            "Epoch 1: validation loss=27.42121, validation acc=0.06719\n",
            "Epoch 2: train loss=17.66710, train acc=0.10594\n",
            "Epoch 2: validation loss=11.22786, validation acc=0.10594\n",
            "Epoch 3: train loss=8.01376, train acc=0.15953\n",
            "Epoch 3: validation loss=4.87893, validation acc=0.15953\n",
            "Epoch 4: train loss=3.23777, train acc=0.15766\n",
            "Epoch 4: validation loss=2.73440, validation acc=0.15766\n",
            "Epoch 5: train loss=2.67351, train acc=0.18094\n",
            "Epoch 5: validation loss=2.65382, validation acc=0.18094\n",
            "Epoch 6: train loss=2.59638, train acc=0.20031\n",
            "Epoch 6: validation loss=2.59780, validation acc=0.20031\n",
            "Epoch 7: train loss=2.52286, train acc=0.21188\n",
            "Epoch 7: validation loss=2.52333, validation acc=0.21188\n",
            "Epoch 8: train loss=2.46838, train acc=0.23781\n",
            "Epoch 8: validation loss=2.48412, validation acc=0.23781\n",
            "Epoch 9: train loss=2.44128, train acc=0.22687\n",
            "Epoch 9: validation loss=2.50347, validation acc=0.22687\n",
            "Epoch 10: train loss=2.40787, train acc=0.25844\n",
            "Epoch 10: validation loss=2.41404, validation acc=0.25844\n",
            "init done for constant tau 0.08\n",
            "Epoch 1: train loss=40.58529, train acc=0.08734\n",
            "Epoch 1: validation loss=12.22421, validation acc=0.08734\n",
            "Epoch 2: train loss=6.91597, train acc=0.07109\n",
            "Epoch 2: validation loss=2.88926, validation acc=0.07109\n",
            "Epoch 3: train loss=2.85972, train acc=0.08609\n",
            "Epoch 3: validation loss=2.83454, validation acc=0.08609\n",
            "Epoch 4: train loss=2.82205, train acc=0.10500\n",
            "Epoch 4: validation loss=2.80650, validation acc=0.10500\n",
            "Epoch 5: train loss=2.78229, train acc=0.11703\n",
            "Epoch 5: validation loss=2.77338, validation acc=0.11703\n",
            "Epoch 6: train loss=2.74131, train acc=0.12531\n",
            "Epoch 6: validation loss=2.71850, validation acc=0.12531\n",
            "Epoch 7: train loss=2.67568, train acc=0.14484\n",
            "Epoch 7: validation loss=2.62878, validation acc=0.14484\n",
            "Epoch 8: train loss=2.59898, train acc=0.17656\n",
            "Epoch 8: validation loss=2.54879, validation acc=0.17656\n",
            "Epoch 9: train loss=2.54988, train acc=0.19141\n",
            "Epoch 9: validation loss=2.55303, validation acc=0.19141\n",
            "Epoch 10: train loss=2.51516, train acc=0.18937\n",
            "Epoch 10: validation loss=2.50309, validation acc=0.18937\n",
            "init done for constant tau 0.02\n",
            "Epoch 1: train loss=62.64042, train acc=0.09578\n",
            "Epoch 1: validation loss=25.21414, validation acc=0.09578\n",
            "Epoch 2: train loss=18.36621, train acc=0.10859\n",
            "Epoch 2: validation loss=12.18353, validation acc=0.10859\n",
            "Epoch 3: train loss=8.47747, train acc=0.15547\n",
            "Epoch 3: validation loss=6.24719, validation acc=0.15547\n",
            "Epoch 4: train loss=4.49656, train acc=0.15031\n",
            "Epoch 4: validation loss=2.75393, validation acc=0.15031\n",
            "Epoch 5: train loss=2.66011, train acc=0.19750\n",
            "Epoch 5: validation loss=2.61443, validation acc=0.19750\n",
            "Epoch 6: train loss=2.54598, train acc=0.22203\n",
            "Epoch 6: validation loss=2.50257, validation acc=0.22203\n",
            "Epoch 7: train loss=2.45757, train acc=0.23359\n",
            "Epoch 7: validation loss=2.47656, validation acc=0.23359\n",
            "Epoch 8: train loss=2.40353, train acc=0.23500\n",
            "Epoch 8: validation loss=2.41976, validation acc=0.23500\n",
            "Epoch 9: train loss=2.34855, train acc=0.28172\n",
            "Epoch 9: validation loss=2.32362, validation acc=0.28172\n",
            "Epoch 10: train loss=2.25561, train acc=0.30844\n",
            "Epoch 10: validation loss=2.26342, validation acc=0.30844\n",
            "init done for constant tau 0.04\n",
            "Epoch 1: train loss=44.52336, train acc=0.11344\n",
            "Epoch 1: validation loss=17.51771, validation acc=0.11344\n",
            "Epoch 2: train loss=10.82051, train acc=0.10141\n",
            "Epoch 2: validation loss=4.27402, validation acc=0.10141\n",
            "Epoch 3: train loss=3.09463, train acc=0.11016\n",
            "Epoch 3: validation loss=2.87399, validation acc=0.11016\n",
            "Epoch 4: train loss=2.81479, train acc=0.13437\n",
            "Epoch 4: validation loss=2.79437, validation acc=0.13437\n",
            "Epoch 5: train loss=2.72918, train acc=0.14563\n",
            "Epoch 5: validation loss=2.71544, validation acc=0.14563\n",
            "Epoch 6: train loss=2.66118, train acc=0.16812\n",
            "Epoch 6: validation loss=2.66093, validation acc=0.16812\n",
            "Epoch 7: train loss=2.61102, train acc=0.17391\n",
            "Epoch 7: validation loss=2.61859, validation acc=0.17391\n",
            "Epoch 8: train loss=2.57778, train acc=0.17750\n",
            "Epoch 8: validation loss=2.59055, validation acc=0.17750\n",
            "Epoch 9: train loss=2.54645, train acc=0.17781\n",
            "Epoch 9: validation loss=2.57563, validation acc=0.17781\n",
            "Epoch 10: train loss=2.51863, train acc=0.18328\n",
            "Epoch 10: validation loss=2.55005, validation acc=0.18328\n",
            "init done for constant tau 0.06\n",
            "Epoch 1: train loss=69.22692, train acc=0.09375\n",
            "Epoch 1: validation loss=30.69123, validation acc=0.09375\n",
            "Epoch 2: train loss=17.66350, train acc=0.15063\n",
            "Epoch 2: validation loss=8.73660, validation acc=0.15063\n",
            "Epoch 3: train loss=4.99746, train acc=0.09344\n",
            "Epoch 3: validation loss=2.84419, validation acc=0.09344\n",
            "Epoch 4: train loss=2.82231, train acc=0.09453\n",
            "Epoch 4: validation loss=2.79149, validation acc=0.09453\n",
            "Epoch 5: train loss=2.78230, train acc=0.10609\n",
            "Epoch 5: validation loss=2.76313, validation acc=0.10609\n",
            "Epoch 6: train loss=2.75398, train acc=0.11922\n",
            "Epoch 6: validation loss=2.74452, validation acc=0.11922\n",
            "Epoch 7: train loss=2.72259, train acc=0.13578\n",
            "Epoch 7: validation loss=2.70418, validation acc=0.13578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bMe0DDphfCv9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f975599b7e9c9973bef0d9ec48ecea55a0d1d2fd74b2454f9f55119bbb3ec555"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('syde552gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "learn_curve_colab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}