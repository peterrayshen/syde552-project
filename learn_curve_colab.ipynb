{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgsYxHYGWkdk",
        "outputId": "2bae5954-0911-4499-ff49-a771b114fce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Password: ··········\n"
          ]
        }
      ],
      "source": [
        "# connect github\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "user = 'peterrayshen'\n",
        "password = getpass('Password: ')\n",
        "repo_name = 'syde552-project'\n",
        "# your password is converted into url format\n",
        "password = urllib.parse.quote(password)\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git'.format(user, password, repo_name)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "# Bad password fails silently so make sure the repo was copied\n",
        "assert os.path.exists(f\"/content/{repo_name}\"), \"Incorrect Password or Repo Not Found, please try again\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"syde552-project/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCh_BAtucaCz",
        "outputId": "d72ef1cc-7200-4183-ce74-8fff617a1ead"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5rk4PtlIWkdo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "import pickle\n",
        "import argparse\n",
        "\n",
        "from utils import SurrGradSpike, get_shd_dataset, sparse_data_generator_from_hdf5_spikes, sparse_data_generator_from_hdf5_spikes_2\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from constants import xp_type_tau_constant, xp_type_tau_gauss, xp_type_tau_uniform, nb_inputs, nb_hidden, nb_outputs, time_step, nb_steps, max_time, batch_size, tau_mem_readout, tau_syn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Sa-nOvg9Wkdp"
      },
      "outputs": [],
      "source": [
        "xp_type = xp_type_tau_constant\n",
        "\n",
        "# general xp params\n",
        "nb_epochs = 2\n",
        "lr = 1e-3\n",
        "num_trials = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if xp_type == xp_type_tau_uniform:\n",
        "    xp_params = {\n",
        "        'uniform_lower_bounds': np.array([38, 35, 30, 20]) * 1e-3,\n",
        "        'uniform_upper_bounds':  np.array([42, 45, 50, 60]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "elif xp_type == xp_type_tau_gauss:\n",
        "    xp_params = {\n",
        "        'gauss_mean': np.array([40, 40, 40, 40]) * 1e-3,\n",
        "        'gauss_std':  np.array([2, 5, 10, 20]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "elif xp_type == xp_type_tau_constant:\n",
        "    xp_params = {\n",
        "        'constant_tau': np.array([20, 40, 60, 80]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "else:\n",
        "    raise ValueError('xp type not recognized')\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Check whether a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")     \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "train_file = h5py.File('data/shd_train.h5', 'r')\n",
        "test_file = h5py.File('data/shd_test.h5', 'r')\n",
        "\n",
        "x_train = train_file['spikes']\n",
        "y_train = train_file['labels']\n",
        "x_test = test_file['spikes']\n",
        "y_test = test_file['labels']\n",
        "\n",
        "x_train_df = pd.DataFrame()\n",
        "x_train_df['times'] = np.array(x_train['times'])\n",
        "x_train_df['units'] = np.array(x_train['units'])\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_train_train, x_train_valid, y_train_train, y_train_valid = train_test_split(x_train_df, y_train_np, test_size=0.2, random_state=42, stratify=y_train_np)\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta_readout    = float(np.exp(-time_step/tau_mem_readout))\n",
        "\n",
        "xp_results = []\n",
        "\n",
        "for trial_num in range(num_trials):\n",
        "    for i in range(xp_params['num_xps']):\n",
        "        if xp_type == xp_type_tau_uniform:\n",
        "            uniform_lower = xp_params['uniform_lower_bounds'][i]\n",
        "            uniform_upper = xp_params['uniform_upper_bounds'][i]\n",
        "            gen_tau_np = np.tile(np.random.uniform(low=uniform_lower, high=uniform_upper, size=nb_hidden), (batch_size, 1))\n",
        "            print(f\"init done for lower bound {uniform_lower}, upper bound {uniform_upper}\")\n",
        "        elif xp_type == xp_type_tau_gauss:\n",
        "            gauss_mean = xp_params['gauss_mean'][i]\n",
        "            gauss_std = xp_params['gauss_std'][i]\n",
        "            gen_tau_np = np.tile(np.random.normal(loc=gauss_mean, scale=gauss_std, size=(nb_hidden)), (batch_size, 1))\n",
        "            print(f\"init done for mean {gauss_mean}, stddev {gauss_std}\")\n",
        "        elif xp_type == xp_type_tau_constant:\n",
        "            constant_tau = xp_params['constant_tau'][i]\n",
        "            gen_tau_np = np.ones((batch_size,nb_hidden)) * constant_tau\n",
        "            print(f\"init done for constant tau {constant_tau}\")\n",
        "        else:\n",
        "            raise ValueError('xp type not recognized')\n",
        "\n",
        "        gen_tau_np[gen_tau_np < 5e-3] = 5e-3\n",
        "        beta_np = np.exp(-time_step/gen_tau_np)\n",
        "        beta_torch = torch.from_numpy(beta_np).float().to(device=device)\n",
        "\n",
        "        weight_scale = 0.2\n",
        "\n",
        "        w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "        w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "        v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(v1, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "        spike_fn  = SurrGradSpike.apply\n",
        "\n",
        "        def run_snn(inputs):\n",
        "            syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "            mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "            mem_rec = []\n",
        "            spk_rec = []\n",
        "\n",
        "            # Compute hidden layer activity\n",
        "            out = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "            h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "            for t in range(nb_steps):\n",
        "                h1 = h1_from_input[:,t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
        "                mthr = mem-1.0\n",
        "                out = spike_fn(mthr)\n",
        "                rst = out.detach() # We do not want to backprop through the reset\n",
        "\n",
        "                new_syn = alpha*syn +h1\n",
        "                new_mem =(beta_torch*mem +syn)*(1.0-rst)\n",
        "\n",
        "                mem_rec.append(mem)\n",
        "                spk_rec.append(out)\n",
        "                \n",
        "                mem = new_mem\n",
        "                syn = new_syn\n",
        "\n",
        "            mem_rec = torch.stack(mem_rec,dim=1)\n",
        "            spk_rec = torch.stack(spk_rec,dim=1)\n",
        "\n",
        "            # Readout layer\n",
        "            h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
        "            flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "            out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "            out_rec = [out]\n",
        "            for t in range(nb_steps):\n",
        "                new_flt = alpha*flt +h2[:,t]\n",
        "                new_out = beta_readout*out +flt\n",
        "\n",
        "                flt = new_flt\n",
        "                out = new_out\n",
        "\n",
        "                out_rec.append(out)\n",
        "\n",
        "            out_rec = torch.stack(out_rec,dim=1)\n",
        "            other_recs = [mem_rec, spk_rec]\n",
        "            return out_rec, other_recs\n",
        "\n",
        "        def compute_classification_accuracy(x_data, y_data):\n",
        "            \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "            accs = []\n",
        "            for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, device, shuffle=False):\n",
        "                output,_ = run_snn(x_local.to_dense())\n",
        "                m,_= torch.max(output,1) # max over time\n",
        "                _,am=torch.max(m,1)      # argmax over output units\n",
        "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "                accs.append(tmp)\n",
        "            return np.mean(accs)\n",
        "\n",
        "        def compute_classification_accuracy_2(x_data, y_data, device):\n",
        "            \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "            accs = []\n",
        "            for x_local, y_local in sparse_data_generator_from_hdf5_spikes_2(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, shuffle=False, device=device):\n",
        "                output,_ = run_snn(x_local.to_dense())\n",
        "                m,_= torch.max(output,1) # max over time\n",
        "                _,am=torch.max(m,1)      # argmax over output units\n",
        "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "                accs.append(tmp)\n",
        "            return np.mean(accs)\n",
        "\n",
        "        def get_mini_batch(x_data, y_data, shuffle=False):\n",
        "            for ret in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, device, shuffle=shuffle):\n",
        "                return ret \n",
        "\n",
        "        def train_with_validation(\n",
        "            x_train_train, \n",
        "            y_train_train,\n",
        "            x_train_valid,\n",
        "            y_train_valid, \n",
        "            lr=1e-3, \n",
        "            nb_epochs=10,\n",
        "            ):\n",
        "            params = [w1,w2,v1]\n",
        "            optimizer = torch.optim.Adamax(params, lr=lr, betas=(0.9,0.999))\n",
        "\n",
        "            log_softmax_fn = nn.LogSoftmax(dim=1)\n",
        "            loss_fn = nn.NLLLoss()\n",
        "            \n",
        "            loss_train_supervised = []\n",
        "            loss_train_reg = []\n",
        "            loss_valid_supervised = []\n",
        "            loss_valid_reg = []\n",
        "            acc_train = []\n",
        "            acc_valid = []\n",
        "\n",
        "            for e in range(nb_epochs):\n",
        "                # train\n",
        "                local_loss_train_supervised = []\n",
        "                local_loss_train_reg = []\n",
        "                for x_local, y_local in sparse_data_generator_from_hdf5_spikes_2(x_train_train, y_train_train, batch_size, nb_steps, nb_inputs, max_time, device):\n",
        "                    output,recs = run_snn(x_local.to_dense())\n",
        "                    _,spks=recs\n",
        "                    m,_=torch.max(output,1)\n",
        "                    log_p_y = log_softmax_fn(m)\n",
        "                    \n",
        "                    # Here we set up our regularizer loss\n",
        "                    # The strength paramters here are merely a guess and there should be ample room for improvement by\n",
        "                    # tuning these paramters.\n",
        "                    reg_loss = 2e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
        "                    reg_loss += 2e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
        "                    \n",
        "                    # Here we combine supervised loss and the regularizer\n",
        "                    loss_val = loss_fn(log_p_y, y_local) + reg_loss\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss_val.backward()\n",
        "                    optimizer.step()\n",
        "                    local_loss_train_supervised.append(loss_fn(log_p_y, y_local).item())\n",
        "                    local_loss_train_reg.append(reg_loss.item())\n",
        "                mean_supervised_loss = np.mean(local_loss_train_supervised)\n",
        "                mean_valid_loss = np.mean(local_loss_train_reg)\n",
        "                loss_train_supervised.append(mean_supervised_loss)\n",
        "                loss_train_reg.append(mean_valid_loss)\n",
        "                local_acc_train = compute_classification_accuracy_2(x_train_train, y_train_train, device)\n",
        "                acc_train.append(local_acc_train)\n",
        "                print(\"Epoch %i: train loss=%.5f, train acc=%.5f\"%(e+1,mean_supervised_loss + mean_valid_loss, local_acc_train))\n",
        "\n",
        "                # validation\n",
        "                local_loss_valid_supervised = []\n",
        "                local_loss_valid_reg = []\n",
        "                for x_local, y_local in sparse_data_generator_from_hdf5_spikes_2(x_train_valid, y_train_valid, batch_size, nb_steps, nb_inputs, max_time, device):\n",
        "                    output,recs = run_snn(x_local.to_dense())\n",
        "                    _,spks=recs\n",
        "                    m,_=torch.max(output,1)\n",
        "                    log_p_y = log_softmax_fn(m)\n",
        "                    \n",
        "                    # Here we set up our regularizer loss\n",
        "                    # The strength paramters here are merely a guess and there should be ample room for improvement by\n",
        "                    # tuning these paramters.\n",
        "                    reg_loss = 2e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
        "                    reg_loss += 2e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
        "                    \n",
        "                    local_loss_valid_supervised.append(loss_fn(log_p_y, y_local).item())\n",
        "                    local_loss_valid_reg.append(reg_loss.item())\n",
        "                mean_supervised_loss = np.mean(local_loss_valid_supervised)\n",
        "                mean_valid_loss = np.mean(local_loss_valid_reg)\n",
        "                loss_valid_supervised.append(mean_supervised_loss)\n",
        "                loss_valid_reg.append(mean_valid_loss)\n",
        "                local_acc_valid = compute_classification_accuracy_2(x_train_valid, y_train_valid, device)\n",
        "                acc_valid.append(local_acc_valid)\n",
        "                print(\"Epoch %i: validation loss=%.5f, validation acc=%.5f\"%(e+1,mean_supervised_loss + mean_valid_loss, local_acc_valid))\n",
        "            \n",
        "            return loss_train_supervised, loss_valid_supervised, loss_train_reg, loss_valid_reg, acc_train, acc_valid\n",
        "\n",
        "        loss_supervised_train, loss_supervised_valid, loss_reg_train, loss_reg_valid, acc_train, acc_valid = train_with_validation(x_train_train, y_train_train, x_train_valid, y_train_valid, lr=lr, nb_epochs=nb_epochs)\n",
        "\n",
        "        x_batch, y_batch = get_mini_batch(x_test, y_test)\n",
        "        output, other_recordings = run_snn(x_batch.to_dense())\n",
        "        mem_rec, spk_rec = other_recordings\n",
        "        acc_test = compute_classification_accuracy(x_test, y_test)\n",
        "\n",
        "        xp_res = {\n",
        "            'w1': w1.cpu().detach().numpy(),\n",
        "            'w2': w2.cpu().detach().numpy(),\n",
        "            'v1': v1.cpu().detach().numpy(),\n",
        "            'weight_scale': weight_scale,\n",
        "            'xp_type': xp_type,\n",
        "            'xp_params': xp_params,\n",
        "            'i': i,\n",
        "            'gen_distribution_tau': gen_tau_np,\n",
        "            'gen_distribution_beta': beta_np,\n",
        "            'alpha': alpha,\n",
        "            'beta_readout': beta_readout,\n",
        "            'nb_inputs': nb_inputs,\n",
        "            'nb_hidden': nb_hidden,\n",
        "            'nb_outputs': nb_outputs,\n",
        "            'time_step': time_step,\n",
        "            'nb_steps': nb_steps,\n",
        "            'max_time': max_time,\n",
        "            'batch_size': batch_size,\n",
        "            'tau_mem_readout': tau_mem_readout,\n",
        "            'tau_syn': tau_syn,\n",
        "            'lr': lr,\n",
        "            'loss_supervised_train': loss_supervised_train,\n",
        "            'loss_supervised_valid': loss_supervised_valid,\n",
        "            'loss_reg_train': loss_reg_train,\n",
        "            'loss_reg_valid': loss_reg_valid,\n",
        "            'acc_test': acc_test,\n",
        "            'acc_train': acc_train,\n",
        "            'acc_valid': acc_valid,\n",
        "            'output': output.cpu().detach().numpy(),\n",
        "            'mem_rec': mem_rec.cpu().detach().numpy(),\n",
        "            'spk_rec': spk_rec.cpu().detach().numpy(),\n",
        "            'trial_num': trial_num,\n",
        "        }\n",
        "        xp_results.append(xp_res)\n",
        "\n",
        "with open(f'/content/gdrive/MyDrive/Colab Notebooks/syde552/learn_curve_{xp_type}.pkl', 'wb') as f:\n",
        "    pickle.dump(xp_results, f)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cgM07N16eMuP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f975599b7e9c9973bef0d9ec48ecea55a0d1d2fd74b2454f9f55119bbb3ec555"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('syde552gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "learn_curve_colab.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}