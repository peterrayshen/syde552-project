{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgsYxHYGWkdk",
        "outputId": "6c02658d-e7d5-4f60-ea12-8ed66e06cb81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Password: ··········\n"
          ]
        }
      ],
      "source": [
        "# connect github\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "user = 'peterrayshen'\n",
        "password = getpass('Password: ')\n",
        "repo_name = 'syde552-project'\n",
        "# your password is converted into url format\n",
        "password = urllib.parse.quote(password)\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git'.format(user, password, repo_name)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "# Bad password fails silently so make sure the repo was copied\n",
        "assert os.path.exists(f\"/content/{repo_name}\"), \"Incorrect Password or Repo Not Found, please try again\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"syde552-project/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCh_BAtucaCz",
        "outputId": "15821a48-a85d-4eff-8aef-3113019b07e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rk4PtlIWkdo",
        "outputId": "5d87bfc9-e59d-45fb-ce40-8e80aca7abf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (1.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install fastprogress\n",
        "from fastprogress import master_bar, progress_bar\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "import pickle\n",
        "import argparse\n",
        "\n",
        "from utils import SurrGradSpike, get_shd_dataset, sparse_data_generator_from_hdf5_spikes, sparse_data_generator_from_hdf5_spikes_2\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from constants import xp_type_tau_constant, xp_type_tau_gauss, xp_type_tau_uniform, nb_inputs, nb_hidden, nb_outputs, time_step, nb_steps, max_time, batch_size, tau_mem_readout, tau_syn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sa-nOvg9Wkdp"
      },
      "outputs": [],
      "source": [
        "xp_type = xp_type_tau_constant\n",
        "\n",
        "# general xp params\n",
        "nb_epochs = 10\n",
        "lr = 1e-3\n",
        "num_trials = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "M_QuxKTUqkdD",
        "outputId": "31e12031-404c-4e54-aab0-717c7da2045b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if xp_type == xp_type_tau_uniform:\n",
        "    xp_params = {\n",
        "        'uniform_lower_bounds': np.array([38, 35, 30, 20]) * 1e-3,\n",
        "        'uniform_upper_bounds':  np.array([42, 45, 50, 60]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "elif xp_type == xp_type_tau_gauss:\n",
        "    xp_params = {\n",
        "        'gauss_mean': np.array([40, 40, 40, 40]) * 1e-3,\n",
        "        'gauss_std':  np.array([2, 5, 10, 20]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "elif xp_type == xp_type_tau_constant:\n",
        "    xp_params = {\n",
        "        'constant_tau': np.array([20, 40, 60, 80]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "else:\n",
        "    raise ValueError('xp type not recognized')\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Check whether a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")     \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "train_file = h5py.File('/content/gdrive/MyDrive/Colab Notebooks/syde552/data/shd_train.h5', 'r')\n",
        "test_file = h5py.File('/content/gdrive/MyDrive/Colab Notebooks/syde552/data/shd_test.h5', 'r')\n",
        "\n",
        "x_train = train_file['spikes']\n",
        "y_train = train_file['labels']\n",
        "x_test = test_file['spikes']\n",
        "y_test = test_file['labels']\n",
        "\n",
        "x_train_df = pd.DataFrame()\n",
        "x_train_df['times'] = np.array(x_train['times'])\n",
        "x_train_df['units'] = np.array(x_train['units'])\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_train_train, x_train_valid, y_train_train, y_train_valid = train_test_split(x_train_df, y_train_np, test_size=0.2, random_state=42, stratify=y_train_np)\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta_readout    = float(np.exp(-time_step/tau_mem_readout))\n",
        "\n",
        "xp_results = []\n",
        "\n",
        "for trial_num in range(num_trials):\n",
        "    for i in range(xp_params['num_xps']):\n",
        "        if xp_type == xp_type_tau_uniform:\n",
        "            uniform_lower = xp_params['uniform_lower_bounds'][i]\n",
        "            uniform_upper = xp_params['uniform_upper_bounds'][i]\n",
        "            gen_tau_np = np.tile(np.random.uniform(low=uniform_lower, high=uniform_upper, size=nb_hidden), (batch_size, 1))\n",
        "            print(f\"init done for lower bound {uniform_lower}, upper bound {uniform_upper}\")\n",
        "        elif xp_type == xp_type_tau_gauss:\n",
        "            gauss_mean = xp_params['gauss_mean'][i]\n",
        "            gauss_std = xp_params['gauss_std'][i]\n",
        "            gen_tau_np = np.tile(np.random.normal(loc=gauss_mean, scale=gauss_std, size=(nb_hidden)), (batch_size, 1))\n",
        "            print(f\"init done for mean {gauss_mean}, stddev {gauss_std}\")\n",
        "        elif xp_type == xp_type_tau_constant:\n",
        "            constant_tau = xp_params['constant_tau'][i]\n",
        "            gen_tau_np = np.ones((batch_size,nb_hidden)) * constant_tau\n",
        "            print(f\"init done for constant tau {constant_tau}\")\n",
        "        else:\n",
        "            raise ValueError('xp type not recognized')\n",
        "\n",
        "        gen_tau_np[gen_tau_np < 5e-3] = 5e-3\n",
        "        beta_np = np.exp(-time_step/gen_tau_np)\n",
        "        beta_torch = torch.from_numpy(beta_np).float().to(device=device)\n",
        "\n",
        "        weight_scale = 0.2\n",
        "\n",
        "        w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "        w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "        v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(v1, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "        spike_fn  = SurrGradSpike.apply\n",
        "\n",
        "        def run_snn(inputs):\n",
        "            syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "            mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "            mem_rec = []\n",
        "            spk_rec = []\n",
        "\n",
        "            # Compute hidden layer activity\n",
        "            out = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "            h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "            for t in range(nb_steps):\n",
        "                h1 = h1_from_input[:,t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
        "                mthr = mem-1.0\n",
        "                out = spike_fn(mthr)\n",
        "                rst = out.detach() # We do not want to backprop through the reset\n",
        "\n",
        "                new_syn = alpha*syn +h1\n",
        "                new_mem =(beta_torch*mem +syn)*(1.0-rst)\n",
        "\n",
        "                mem_rec.append(mem)\n",
        "                spk_rec.append(out)\n",
        "                \n",
        "                mem = new_mem\n",
        "                syn = new_syn\n",
        "\n",
        "            mem_rec = torch.stack(mem_rec,dim=1)\n",
        "            spk_rec = torch.stack(spk_rec,dim=1)\n",
        "\n",
        "            # Readout layer\n",
        "            h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
        "            flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "            out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "            out_rec = [out]\n",
        "            for t in range(nb_steps):\n",
        "                new_flt = alpha*flt +h2[:,t]\n",
        "                new_out = beta_readout*out +flt\n",
        "\n",
        "                flt = new_flt\n",
        "                out = new_out\n",
        "\n",
        "                out_rec.append(out)\n",
        "\n",
        "            out_rec = torch.stack(out_rec,dim=1)\n",
        "            other_recs = [mem_rec, spk_rec]\n",
        "            return out_rec, other_recs\n",
        "\n",
        "        def compute_classification_accuracy(x_data, y_data):\n",
        "            \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "            accs = []\n",
        "            for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, device, shuffle=False):\n",
        "                output,_ = run_snn(x_local.to_dense())\n",
        "                m,_= torch.max(output,1) # max over time\n",
        "                _,am=torch.max(m,1)      # argmax over output units\n",
        "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "                accs.append(tmp)\n",
        "            return np.mean(accs)\n",
        "\n",
        "        def compute_classification_accuracy_2(batch_cache, device):\n",
        "            \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "            accs = []\n",
        "            for x_local, y_local in batch_cache:\n",
        "                output,_ = run_snn(x_local.to_dense())\n",
        "                m,_= torch.max(output,1) # max over time\n",
        "                _,am=torch.max(m,1)      # argmax over output units\n",
        "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "                accs.append(tmp)\n",
        "            return np.mean(accs)\n",
        "\n",
        "        def get_mini_batch(x_data, y_data, shuffle=False):\n",
        "            for ret in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, device, shuffle=shuffle):\n",
        "                return ret \n",
        "\n",
        "        def train_with_validation(\n",
        "            x_train_train, \n",
        "            y_train_train,\n",
        "            x_train_valid,\n",
        "            y_train_valid, \n",
        "            lr=1e-3, \n",
        "            nb_epochs=10,\n",
        "            ):\n",
        "            params = [w1,w2,v1]\n",
        "            optimizer = torch.optim.Adamax(params, lr=lr, betas=(0.9,0.999))\n",
        "\n",
        "            log_softmax_fn = nn.LogSoftmax(dim=1)\n",
        "            loss_fn = nn.NLLLoss()\n",
        "            \n",
        "            loss_train_supervised = []\n",
        "            loss_train_reg = []\n",
        "            loss_valid_supervised = []\n",
        "            loss_valid_reg = []\n",
        "            acc_train = []\n",
        "            acc_valid = []\n",
        "\n",
        "            batch_cache_train_train = list(sparse_data_generator_from_hdf5_spikes_2(x_train_train, y_train_train, batch_size, nb_steps, nb_inputs, max_time, device))\n",
        "            batch_cache_train_valid = list(sparse_data_generator_from_hdf5_spikes_2(x_train_valid, y_train_valid, batch_size, nb_steps, nb_inputs, max_time, device))\n",
        "\n",
        "            for e in range(nb_epochs):\n",
        "                # train\n",
        "                local_loss_train_supervised = []\n",
        "                local_loss_train_reg = []\n",
        "                for x_local, y_local in batch_cache_train_train:\n",
        "                    output,recs = run_snn(x_local.to_dense())\n",
        "                    _,spks=recs\n",
        "                    m,_=torch.max(output,1)\n",
        "                    log_p_y = log_softmax_fn(m)\n",
        "                    \n",
        "                    # Here we set up our regularizer loss\n",
        "                    # The strength paramters here are merely a guess and there should be ample room for improvement by\n",
        "                    # tuning these paramters.\n",
        "                    reg_loss = 2e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
        "                    reg_loss += 2e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
        "                    \n",
        "                    # Here we combine supervised loss and the regularizer\n",
        "                    loss_val = loss_fn(log_p_y, y_local) + reg_loss\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss_val.backward()\n",
        "                    optimizer.step()\n",
        "                    local_loss_train_supervised.append(loss_fn(log_p_y, y_local).item())\n",
        "                    local_loss_train_reg.append(reg_loss.item())\n",
        "                mean_supervised_loss = np.mean(local_loss_train_supervised)\n",
        "                mean_valid_loss = np.mean(local_loss_train_reg)\n",
        "                loss_train_supervised.append(mean_supervised_loss)\n",
        "                loss_train_reg.append(mean_valid_loss)\n",
        "                local_acc_train = compute_classification_accuracy_2(batch_cache_train_train, device)\n",
        "                acc_train.append(local_acc_train)\n",
        "                print(\"Epoch %i: train loss=%.5f, train acc=%.5f\"%(e+1,mean_supervised_loss + mean_valid_loss, local_acc_train))\n",
        "\n",
        "                # validation\n",
        "                local_loss_valid_supervised = []\n",
        "                local_loss_valid_reg = []\n",
        "                for x_local, y_local in batch_cache_train_valid:\n",
        "                    output,recs = run_snn(x_local.to_dense())\n",
        "                    _,spks=recs\n",
        "                    m,_=torch.max(output,1)\n",
        "                    log_p_y = log_softmax_fn(m)\n",
        "                    \n",
        "                    # Here we set up our regularizer loss\n",
        "                    # The strength paramters here are merely a guess and there should be ample room for improvement by\n",
        "                    # tuning these paramters.\n",
        "                    reg_loss = 2e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
        "                    reg_loss += 2e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
        "                    \n",
        "                    local_loss_valid_supervised.append(loss_fn(log_p_y, y_local).item())\n",
        "                    local_loss_valid_reg.append(reg_loss.item())\n",
        "                mean_supervised_loss = np.mean(local_loss_valid_supervised)\n",
        "                mean_valid_loss = np.mean(local_loss_valid_reg)\n",
        "                loss_valid_supervised.append(mean_supervised_loss)\n",
        "                loss_valid_reg.append(mean_valid_loss)\n",
        "                local_acc_valid = compute_classification_accuracy_2(batch_cache_train_train, device)\n",
        "                acc_valid.append(local_acc_valid)\n",
        "                print(\"Epoch %i: validation loss=%.5f, validation acc=%.5f\"%(e+1,mean_supervised_loss + mean_valid_loss, local_acc_valid))\n",
        "            \n",
        "            return loss_train_supervised, loss_valid_supervised, loss_train_reg, loss_valid_reg, acc_train, acc_valid\n",
        "\n",
        "        loss_supervised_train, loss_supervised_valid, loss_reg_train, loss_reg_valid, acc_train, acc_valid = train_with_validation(x_train_train, y_train_train, x_train_valid, y_train_valid, lr=lr, nb_epochs=nb_epochs)\n",
        "\n",
        "        x_batch, y_batch = get_mini_batch(x_test, y_test)\n",
        "        output, other_recordings = run_snn(x_batch.to_dense())\n",
        "        mem_rec, spk_rec = other_recordings\n",
        "        acc_test = compute_classification_accuracy(x_test, y_test)\n",
        "\n",
        "        xp_res = {\n",
        "            'w1': w1.cpu().detach().numpy(),\n",
        "            'w2': w2.cpu().detach().numpy(),\n",
        "            'v1': v1.cpu().detach().numpy(),\n",
        "            'weight_scale': weight_scale,\n",
        "            'xp_type': xp_type,\n",
        "            'xp_params': xp_params,\n",
        "            'i': i,\n",
        "            'gen_distribution_tau': gen_tau_np,\n",
        "            'gen_distribution_beta': beta_np,\n",
        "            'alpha': alpha,\n",
        "            'beta_readout': beta_readout,\n",
        "            'nb_inputs': nb_inputs,\n",
        "            'nb_hidden': nb_hidden,\n",
        "            'nb_outputs': nb_outputs,\n",
        "            'time_step': time_step,\n",
        "            'nb_steps': nb_steps,\n",
        "            'max_time': max_time,\n",
        "            'batch_size': batch_size,\n",
        "            'tau_mem_readout': tau_mem_readout,\n",
        "            'tau_syn': tau_syn,\n",
        "            'lr': lr,\n",
        "            'loss_supervised_train': loss_supervised_train,\n",
        "            'loss_supervised_valid': loss_supervised_valid,\n",
        "            'loss_reg_train': loss_reg_train,\n",
        "            'loss_reg_valid': loss_reg_valid,\n",
        "            'acc_test': acc_test,\n",
        "            'acc_train': acc_train,\n",
        "            'acc_valid': acc_valid,\n",
        "            'output': output.cpu().detach().numpy(),\n",
        "            'mem_rec': mem_rec.cpu().detach().numpy(),\n",
        "            'spk_rec': spk_rec.cpu().detach().numpy(),\n",
        "            'trial_num': trial_num,\n",
        "        }\n",
        "        xp_results.append(xp_res)\n",
        "\n",
        "with open(f'/content/gdrive/MyDrive/Colab Notebooks/syde552/learn_curve_{xp_type}.pkl', 'wb') as f:\n",
        "    pickle.dump(xp_results, f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "cgM07N16eMuP",
        "outputId": "a2e870bc-9f64-4f91-e509-26b0dcaefcdb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-518947c8ae85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Using device: {device}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/shd_train.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/shd_test.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n\u001b[0;32m--> 427\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'data/shd_train.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bMe0DDphfCv9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f975599b7e9c9973bef0d9ec48ecea55a0d1d2fd74b2454f9f55119bbb3ec555"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('syde552gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "learn_curve_colab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}