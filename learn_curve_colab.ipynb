{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgsYxHYGWkdk",
        "outputId": "6c02658d-e7d5-4f60-ea12-8ed66e06cb81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Password: ··········\n"
          ]
        }
      ],
      "source": [
        "# connect github\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "user = 'peterrayshen'\n",
        "password = getpass('Password: ')\n",
        "repo_name = 'syde552-project'\n",
        "# your password is converted into url format\n",
        "password = urllib.parse.quote(password)\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git'.format(user, password, repo_name)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "# Bad password fails silently so make sure the repo was copied\n",
        "assert os.path.exists(f\"/content/{repo_name}\"), \"Incorrect Password or Repo Not Found, please try again\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"syde552-project/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCh_BAtucaCz",
        "outputId": "15821a48-a85d-4eff-8aef-3113019b07e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rk4PtlIWkdo",
        "outputId": "5d87bfc9-e59d-45fb-ce40-8e80aca7abf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (1.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install fastprogress\n",
        "from fastprogress import master_bar, progress_bar\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "import pickle\n",
        "import argparse\n",
        "\n",
        "from utils import SurrGradSpike, get_shd_dataset, sparse_data_generator_from_hdf5_spikes, sparse_data_generator_from_hdf5_spikes_2\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from constants import xp_type_tau_constant, xp_type_tau_gauss, xp_type_tau_uniform, nb_inputs, nb_hidden, nb_outputs, time_step, nb_steps, max_time, batch_size, tau_mem_readout, tau_syn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Sa-nOvg9Wkdp"
      },
      "outputs": [],
      "source": [
        "xp_type = xp_type_tau_constant\n",
        "\n",
        "# general xp params\n",
        "nb_epochs = 200\n",
        "lr = 1e-3\n",
        "num_trials = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "M_QuxKTUqkdD",
        "outputId": "ac1d07ac-f211-4a53-f5fd-1ef8148fac0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if xp_type == xp_type_tau_uniform:\n",
        "    xp_params = {\n",
        "        'uniform_lower_bounds': np.array([38, 35, 30, 20]) * 1e-3,\n",
        "        'uniform_upper_bounds':  np.array([42, 45, 50, 60]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "elif xp_type == xp_type_tau_gauss:\n",
        "    xp_params = {\n",
        "        'gauss_mean': np.array([40, 40, 40, 40]) * 1e-3,\n",
        "        'gauss_std':  np.array([2, 5, 10, 20]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "elif xp_type == xp_type_tau_constant:\n",
        "    xp_params = {\n",
        "        'constant_tau': np.array([20, 40, 60, 80]) * 1e-3,\n",
        "        'num_xps': 4,\n",
        "    }\n",
        "else:\n",
        "    raise ValueError('xp type not recognized')\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Check whether a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")     \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "train_file = h5py.File('data/shd_train.h5', 'r')\n",
        "test_file = h5py.File('data/shd_test.h5', 'r')\n",
        "\n",
        "x_train = train_file['spikes']\n",
        "y_train = train_file['labels']\n",
        "x_test = test_file['spikes']\n",
        "y_test = test_file['labels']\n",
        "\n",
        "x_train_df = pd.DataFrame()\n",
        "x_train_df['times'] = np.array(x_train['times'])\n",
        "x_train_df['units'] = np.array(x_train['units'])\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "x_train_train, x_train_valid, y_train_train, y_train_valid = train_test_split(x_train_df, y_train_np, test_size=0.2, random_state=42, stratify=y_train_np)\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta_readout    = float(np.exp(-time_step/tau_mem_readout))\n",
        "\n",
        "xp_results = []\n",
        "\n",
        "for trial_num in range(num_trials):\n",
        "    for i in range(xp_params['num_xps']):\n",
        "        if xp_type == xp_type_tau_uniform:\n",
        "            uniform_lower = xp_params['uniform_lower_bounds'][i]\n",
        "            uniform_upper = xp_params['uniform_upper_bounds'][i]\n",
        "            gen_tau_np = np.tile(np.random.uniform(low=uniform_lower, high=uniform_upper, size=nb_hidden), (batch_size, 1))\n",
        "            print(f\"init done for lower bound {uniform_lower}, upper bound {uniform_upper}\")\n",
        "        elif xp_type == xp_type_tau_gauss:\n",
        "            gauss_mean = xp_params['gauss_mean'][i]\n",
        "            gauss_std = xp_params['gauss_std'][i]\n",
        "            gen_tau_np = np.tile(np.random.normal(loc=gauss_mean, scale=gauss_std, size=(nb_hidden)), (batch_size, 1))\n",
        "            print(f\"init done for mean {gauss_mean}, stddev {gauss_std}\")\n",
        "        elif xp_type == xp_type_tau_constant:\n",
        "            constant_tau = xp_params['constant_tau'][i]\n",
        "            gen_tau_np = np.ones((batch_size,nb_hidden)) * constant_tau\n",
        "            print(f\"init done for constant tau {constant_tau}\")\n",
        "        else:\n",
        "            raise ValueError('xp type not recognized')\n",
        "\n",
        "        gen_tau_np[gen_tau_np < 5e-3] = 5e-3\n",
        "        beta_np = np.exp(-time_step/gen_tau_np)\n",
        "        beta_torch = torch.from_numpy(beta_np).float().to(device=device)\n",
        "\n",
        "        weight_scale = 0.2\n",
        "\n",
        "        w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "        w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "        v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
        "        torch.nn.init.normal_(v1, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "        spike_fn  = SurrGradSpike.apply\n",
        "\n",
        "        def run_snn(inputs):\n",
        "            syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "            mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "            mem_rec = []\n",
        "            spk_rec = []\n",
        "\n",
        "            # Compute hidden layer activity\n",
        "            out = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "            h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "            for t in range(nb_steps):\n",
        "                h1 = h1_from_input[:,t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
        "                mthr = mem-1.0\n",
        "                out = spike_fn(mthr)\n",
        "                rst = out.detach() # We do not want to backprop through the reset\n",
        "\n",
        "                new_syn = alpha*syn +h1\n",
        "                new_mem =(beta_torch*mem +syn)*(1.0-rst)\n",
        "\n",
        "                mem_rec.append(mem)\n",
        "                spk_rec.append(out)\n",
        "                \n",
        "                mem = new_mem\n",
        "                syn = new_syn\n",
        "\n",
        "            mem_rec = torch.stack(mem_rec,dim=1)\n",
        "            spk_rec = torch.stack(spk_rec,dim=1)\n",
        "\n",
        "            # Readout layer\n",
        "            h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
        "            flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "            out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "            out_rec = [out]\n",
        "            for t in range(nb_steps):\n",
        "                new_flt = alpha*flt +h2[:,t]\n",
        "                new_out = beta_readout*out +flt\n",
        "\n",
        "                flt = new_flt\n",
        "                out = new_out\n",
        "\n",
        "                out_rec.append(out)\n",
        "\n",
        "            out_rec = torch.stack(out_rec,dim=1)\n",
        "            other_recs = [mem_rec, spk_rec]\n",
        "            return out_rec, other_recs\n",
        "\n",
        "        def compute_classification_accuracy(x_data, y_data):\n",
        "            \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "            accs = []\n",
        "            for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, device, shuffle=False):\n",
        "                output,_ = run_snn(x_local.to_dense())\n",
        "                m,_= torch.max(output,1) # max over time\n",
        "                _,am=torch.max(m,1)      # argmax over output units\n",
        "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "                accs.append(tmp)\n",
        "            return np.mean(accs)\n",
        "\n",
        "        def compute_classification_accuracy_2(batch_cache, device):\n",
        "            \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "            accs = []\n",
        "            for x_local, y_local in batch_cache:\n",
        "                output,_ = run_snn(x_local.to_dense())\n",
        "                m,_= torch.max(output,1) # max over time\n",
        "                _,am=torch.max(m,1)      # argmax over output units\n",
        "                tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "                accs.append(tmp)\n",
        "            return np.mean(accs)\n",
        "\n",
        "        def get_mini_batch(x_data, y_data, shuffle=False):\n",
        "            for ret in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, device, shuffle=shuffle):\n",
        "                return ret \n",
        "\n",
        "        def train_with_validation(\n",
        "            x_train_train, \n",
        "            y_train_train,\n",
        "            x_train_valid,\n",
        "            y_train_valid, \n",
        "            lr=1e-3, \n",
        "            nb_epochs=10,\n",
        "            ):\n",
        "            params = [w1,w2,v1]\n",
        "            optimizer = torch.optim.Adamax(params, lr=lr, betas=(0.9,0.999))\n",
        "\n",
        "            log_softmax_fn = nn.LogSoftmax(dim=1)\n",
        "            loss_fn = nn.NLLLoss()\n",
        "            \n",
        "            loss_train_supervised = []\n",
        "            loss_train_reg = []\n",
        "            loss_valid_supervised = []\n",
        "            loss_valid_reg = []\n",
        "            acc_train = []\n",
        "            acc_valid = []\n",
        "\n",
        "            batch_cache_train_train = list(sparse_data_generator_from_hdf5_spikes_2(x_train_train, y_train_train, batch_size, nb_steps, nb_inputs, max_time, device))\n",
        "            batch_cache_train_valid = list(sparse_data_generator_from_hdf5_spikes_2(x_train_valid, y_train_valid, batch_size, nb_steps, nb_inputs, max_time, device))\n",
        "\n",
        "            for e in range(nb_epochs):\n",
        "                # train\n",
        "                local_loss_train_supervised = []\n",
        "                local_loss_train_reg = []\n",
        "                for x_local, y_local in batch_cache_train_train:\n",
        "                    output,recs = run_snn(x_local.to_dense())\n",
        "                    _,spks=recs\n",
        "                    m,_=torch.max(output,1)\n",
        "                    log_p_y = log_softmax_fn(m)\n",
        "                    \n",
        "                    # Here we set up our regularizer loss\n",
        "                    # The strength paramters here are merely a guess and there should be ample room for improvement by\n",
        "                    # tuning these paramters.\n",
        "                    reg_loss = 2e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
        "                    reg_loss += 2e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
        "                    \n",
        "                    # Here we combine supervised loss and the regularizer\n",
        "                    loss_val = loss_fn(log_p_y, y_local) + reg_loss\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss_val.backward()\n",
        "                    optimizer.step()\n",
        "                    local_loss_train_supervised.append(loss_fn(log_p_y, y_local).item())\n",
        "                    local_loss_train_reg.append(reg_loss.item())\n",
        "                mean_supervised_loss = np.mean(local_loss_train_supervised)\n",
        "                mean_valid_loss = np.mean(local_loss_train_reg)\n",
        "                loss_train_supervised.append(mean_supervised_loss)\n",
        "                loss_train_reg.append(mean_valid_loss)\n",
        "                local_acc_train = compute_classification_accuracy_2(batch_cache_train_train, device)\n",
        "                acc_train.append(local_acc_train)\n",
        "                print(\"Epoch %i: train loss=%.5f, train acc=%.5f\"%(e+1,mean_supervised_loss + mean_valid_loss, local_acc_train))\n",
        "\n",
        "                # validation\n",
        "                local_loss_valid_supervised = []\n",
        "                local_loss_valid_reg = []\n",
        "                for x_local, y_local in batch_cache_train_valid:\n",
        "                    output,recs = run_snn(x_local.to_dense())\n",
        "                    _,spks=recs\n",
        "                    m,_=torch.max(output,1)\n",
        "                    log_p_y = log_softmax_fn(m)\n",
        "                    \n",
        "                    # Here we set up our regularizer loss\n",
        "                    # The strength paramters here are merely a guess and there should be ample room for improvement by\n",
        "                    # tuning these paramters.\n",
        "                    reg_loss = 2e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
        "                    reg_loss += 2e-6*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
        "                    \n",
        "                    local_loss_valid_supervised.append(loss_fn(log_p_y, y_local).item())\n",
        "                    local_loss_valid_reg.append(reg_loss.item())\n",
        "                mean_supervised_loss = np.mean(local_loss_valid_supervised)\n",
        "                mean_valid_loss = np.mean(local_loss_valid_reg)\n",
        "                loss_valid_supervised.append(mean_supervised_loss)\n",
        "                loss_valid_reg.append(mean_valid_loss)\n",
        "                local_acc_valid = compute_classification_accuracy_2(batch_cache_train_train, device)\n",
        "                acc_valid.append(local_acc_valid)\n",
        "                print(\"Epoch %i: validation loss=%.5f, validation acc=%.5f\"%(e+1,mean_supervised_loss + mean_valid_loss, local_acc_valid))\n",
        "            \n",
        "            return loss_train_supervised, loss_valid_supervised, loss_train_reg, loss_valid_reg, acc_train, acc_valid\n",
        "\n",
        "        loss_supervised_train, loss_supervised_valid, loss_reg_train, loss_reg_valid, acc_train, acc_valid = train_with_validation(x_train_train, y_train_train, x_train_valid, y_train_valid, lr=lr, nb_epochs=nb_epochs)\n",
        "\n",
        "        x_batch, y_batch = get_mini_batch(x_test, y_test)\n",
        "        output, other_recordings = run_snn(x_batch.to_dense())\n",
        "        mem_rec, spk_rec = other_recordings\n",
        "        acc_test = compute_classification_accuracy(x_test, y_test)\n",
        "\n",
        "        xp_res = {\n",
        "            'w1': w1.cpu().detach().numpy(),\n",
        "            'w2': w2.cpu().detach().numpy(),\n",
        "            'v1': v1.cpu().detach().numpy(),\n",
        "            'weight_scale': weight_scale,\n",
        "            'xp_type': xp_type,\n",
        "            'xp_params': xp_params,\n",
        "            'i': i,\n",
        "            'gen_distribution_tau': gen_tau_np,\n",
        "            'gen_distribution_beta': beta_np,\n",
        "            'alpha': alpha,\n",
        "            'beta_readout': beta_readout,\n",
        "            'nb_inputs': nb_inputs,\n",
        "            'nb_hidden': nb_hidden,\n",
        "            'nb_outputs': nb_outputs,\n",
        "            'time_step': time_step,\n",
        "            'nb_steps': nb_steps,\n",
        "            'max_time': max_time,\n",
        "            'batch_size': batch_size,\n",
        "            'tau_mem_readout': tau_mem_readout,\n",
        "            'tau_syn': tau_syn,\n",
        "            'lr': lr,\n",
        "            'loss_supervised_train': loss_supervised_train,\n",
        "            'loss_supervised_valid': loss_supervised_valid,\n",
        "            'loss_reg_train': loss_reg_train,\n",
        "            'loss_reg_valid': loss_reg_valid,\n",
        "            'acc_test': acc_test,\n",
        "            'acc_train': acc_train,\n",
        "            'acc_valid': acc_valid,\n",
        "            'output': output.cpu().detach().numpy(),\n",
        "            'mem_rec': mem_rec.cpu().detach().numpy(),\n",
        "            'spk_rec': spk_rec.cpu().detach().numpy(),\n",
        "            'trial_num': trial_num,\n",
        "        }\n",
        "        xp_results.append(xp_res)\n",
        "\n",
        "with open(f'results/learn_curve_{xp_type}.pkl', 'wb') as f:\n",
        "    pickle.dump(xp_results, f)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cgM07N16eMuP",
        "outputId": "4048e152-00fa-4551-9d77-2a1c6be8a7ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/5 00:00<00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/4 00:00<00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init done for constant tau 0.02\n",
            "Epoch 1: train loss=54.21922, train acc=0.07562\n",
            "Epoch 1: validation loss=21.05358, validation acc=0.07096\n",
            "Epoch 2: train loss=11.92944, train acc=0.13422\n",
            "Epoch 2: validation loss=6.74001, validation acc=0.14323\n",
            "Epoch 3: train loss=4.81667, train acc=0.14516\n",
            "Epoch 3: validation loss=3.87074, validation acc=0.13346\n",
            "Epoch 4: train loss=3.02376, train acc=0.15578\n",
            "Epoch 4: validation loss=2.73178, validation acc=0.14388\n",
            "Epoch 5: train loss=2.64127, train acc=0.18484\n",
            "Epoch 5: validation loss=2.57109, validation acc=0.17122\n",
            "Epoch 6: train loss=2.48522, train acc=0.22703\n",
            "Epoch 6: validation loss=2.47520, validation acc=0.22331\n",
            "Epoch 7: train loss=2.41439, train acc=0.26109\n",
            "Epoch 7: validation loss=2.37795, validation acc=0.25456\n",
            "Epoch 8: train loss=2.34583, train acc=0.28984\n",
            "Epoch 8: validation loss=2.33662, validation acc=0.26562\n",
            "Epoch 9: train loss=2.29520, train acc=0.29547\n",
            "Epoch 9: validation loss=2.31259, validation acc=0.28255\n",
            "Epoch 10: train loss=2.24234, train acc=0.34828\n",
            "Epoch 10: validation loss=2.24714, validation acc=0.32943\n",
            "Epoch 11: train loss=2.21234, train acc=0.33578\n",
            "Epoch 11: validation loss=2.25733, validation acc=0.32682\n",
            "Epoch 12: train loss=2.18499, train acc=0.35281\n",
            "Epoch 12: validation loss=2.23007, validation acc=0.30924\n",
            "Epoch 13: train loss=2.12987, train acc=0.35406\n",
            "Epoch 13: validation loss=2.15948, validation acc=0.32747\n",
            "Epoch 14: train loss=2.08901, train acc=0.35437\n",
            "Epoch 14: validation loss=2.16646, validation acc=0.32747\n",
            "Epoch 15: train loss=2.07525, train acc=0.36062\n",
            "Epoch 15: validation loss=2.12993, validation acc=0.32943\n",
            "Epoch 16: train loss=2.00981, train acc=0.40578\n",
            "Epoch 16: validation loss=2.03791, validation acc=0.38411\n",
            "Epoch 17: train loss=1.95566, train acc=0.41719\n",
            "Epoch 17: validation loss=1.97472, validation acc=0.39648\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9544f8edf52f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss_train_supervised\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_valid_supervised\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_valid_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mloss_supervised_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_supervised_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_reg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_reg_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9544f8edf52f>\u001b[0m in \u001b[0;36mtrain_with_validation\u001b[0;34m(x_train_train, y_train_train, x_train_valid, y_train_valid, lr, nb_epochs)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mlocal_loss_train_supervised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mlocal_loss_train_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_local\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msparse_data_generator_from_hdf5_spikes_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_snn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/syde552-project/utils.py\u001b[0m in \u001b[0;36msparse_data_generator_from_hdf5_spikes_2\u001b[0;34m(X, y, batch_size, nb_steps, nb_units, max_time, device, shuffle)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mcoo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiring_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits_fired\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdigitize\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdigitize\u001b[0;34m(x, bins, right)\u001b[0m\n\u001b[1;32m   4930\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4931\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msearchsorted\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \"\"\"\n\u001b[0;32m-> 1350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'searchsorted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bMe0DDphfCv9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f975599b7e9c9973bef0d9ec48ecea55a0d1d2fd74b2454f9f55119bbb3ec555"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('syde552gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "learn_curve_colab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}